{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline - Should this be a challenge?\n",
    "\n",
    "**[Big Data and Cloud Computing]**\n",
    "\n",
    "## Group D\n",
    "* Daniela Tom√°s, up202004946\n",
    "* Diogo Nunes, up202007895\n",
    "* Diogo Almeida, up202006059"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://stackoverflow.com/questions/59659344/how-to-process-faster-on-gz-files-in-spark-scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding and Preparation\n",
    "\n",
    "Firstly, we import the necessary libraries, packages and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is too large (4.2 Gigabytes compressed), we load it into Spark. However, it is inefficient to process gzip-compressed CSV files directly with Spark due to their non-splittable nature, and using an unziped CSV file is not always splittable. As shown in the code below, the CSV file took over 7 minutes to run, which is a considerable time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ROW_ID: integer (nullable = true)\n",
      " |-- SUBJECT_ID: integer (nullable = true)\n",
      " |-- HADM_ID: integer (nullable = true)\n",
      " |-- ICUSTAY_ID: integer (nullable = true)\n",
      " |-- ITEMID: integer (nullable = true)\n",
      " |-- CHARTTIME: timestamp (nullable = true)\n",
      " |-- STORETIME: timestamp (nullable = true)\n",
      " |-- CGID: integer (nullable = true)\n",
      " |-- VALUE: string (nullable = true)\n",
      " |-- VALUENUM: double (nullable = true)\n",
      " |-- VALUEUOM: string (nullable = true)\n",
      " |-- WARNING: integer (nullable = true)\n",
      " |-- ERROR: integer (nullable = true)\n",
      " |-- RESULTSTATUS: string (nullable = true)\n",
      " |-- STOPPED: string (nullable = true)\n",
      "\n",
      "+------+----------+-------+----------+------+-------------------+-------------------+-----+-----+--------+--------+-------+-----+------------+-------+\n",
      "|ROW_ID|SUBJECT_ID|HADM_ID|ICUSTAY_ID|ITEMID|          CHARTTIME|          STORETIME| CGID|VALUE|VALUENUM|VALUEUOM|WARNING|ERROR|RESULTSTATUS|STOPPED|\n",
      "+------+----------+-------+----------+------+-------------------+-------------------+-----+-----+--------+--------+-------+-----+------------+-------+\n",
      "|   788|        36| 165660|    241249|223834|2134-05-12 12:00:00|2134-05-12 13:56:00|17525|   15|    15.0|   L/min|      0|    0|        NULL|   NULL|\n",
      "|   789|        36| 165660|    241249|223835|2134-05-12 12:00:00|2134-05-12 13:56:00|17525|  100|   100.0|    NULL|      0|    0|        NULL|   NULL|\n",
      "|   790|        36| 165660|    241249|224328|2134-05-12 12:00:00|2134-05-12 12:18:00|20823|  .37|    0.37|    NULL|      0|    0|        NULL|   NULL|\n",
      "|   791|        36| 165660|    241249|224329|2134-05-12 12:00:00|2134-05-12 12:19:00|20823|    6|     6.0|     min|      0|    0|        NULL|   NULL|\n",
      "|   792|        36| 165660|    241249|224330|2134-05-12 12:00:00|2134-05-12 12:19:00|20823|  2.5|     2.5|    NULL|      0|    0|        NULL|   NULL|\n",
      "+------+----------+-------+----------+------+-------------------+-------------------+-----+-----+--------+--------+-------+-----+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "file_path = \"dataset/CHARTEVENTS.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve performance, we load the dataset into Spark using the Parquet file format with Snappy compression, ensuring splittable and efficient parallel processing across multiple nodes in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file_path = \"dataset/CHARTEVENTS.parquet\"\n",
    "\n",
    "df.write.format(\"parquet\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .save(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ROW_ID: integer (nullable = true)\n",
      " |-- SUBJECT_ID: integer (nullable = true)\n",
      " |-- HADM_ID: integer (nullable = true)\n",
      " |-- ICUSTAY_ID: integer (nullable = true)\n",
      " |-- ITEMID: integer (nullable = true)\n",
      " |-- CHARTTIME: timestamp (nullable = true)\n",
      " |-- STORETIME: timestamp (nullable = true)\n",
      " |-- CGID: integer (nullable = true)\n",
      " |-- VALUE: string (nullable = true)\n",
      " |-- VALUENUM: double (nullable = true)\n",
      " |-- VALUEUOM: string (nullable = true)\n",
      " |-- WARNING: integer (nullable = true)\n",
      " |-- ERROR: integer (nullable = true)\n",
      " |-- RESULTSTATUS: string (nullable = true)\n",
      " |-- STOPPED: string (nullable = true)\n",
      "\n",
      "+-------+----------+-------+----------+------+-------------------+-------------------+-----+-----+--------+--------+-------+-----+------------+-------+\n",
      "| ROW_ID|SUBJECT_ID|HADM_ID|ICUSTAY_ID|ITEMID|          CHARTTIME|          STORETIME| CGID|VALUE|VALUENUM|VALUEUOM|WARNING|ERROR|RESULTSTATUS|STOPPED|\n",
      "+-------+----------+-------+----------+------+-------------------+-------------------+-----+-----+--------+--------+-------+-----+------------+-------+\n",
      "|2876208|     24548| 114085|    276527|226253|2191-01-28 20:15:00|2191-01-28 20:16:00|20868|   90|    90.0|       %|      0|    0|        NULL|   NULL|\n",
      "|2876209|     24548| 114085|    276527|223834|2191-01-28 20:55:00|2191-01-28 20:55:00|20868|    3|     3.0|   L/min|      0|    0|        NULL|   NULL|\n",
      "|2876210|     24548| 114085|    276527|220045|2191-01-28 21:00:00|2191-01-28 21:09:00|20868|   87|    87.0|     bpm|      0|    0|        NULL|   NULL|\n",
      "|2876211|     24548| 114085|    276527|220179|2191-01-28 21:00:00|2191-01-28 21:09:00|20868|  179|   179.0|    mmHg|      0|    0|        NULL|   NULL|\n",
      "|2876212|     24548| 114085|    276527|220180|2191-01-28 21:00:00|2191-01-28 21:09:00|20868|  114|   114.0|    mmHg|      0|    0|        NULL|   NULL|\n",
      "+-------+----------+-------+----------+------+-------------------+-------------------+-----+-----+--------+--------+-------+-----+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"dataset/CHARTEVENTS.parquet\"\n",
    "\n",
    "df = spark.read.format(\"parquet\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
