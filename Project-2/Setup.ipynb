{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can be configured with lots of settings.  \n",
    "In particular we highlight the \"local[*]\" which configures spark to run in standalone mode, so everything is running in the local machine, using all available threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Setup\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.host\",\"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.bindAddress\",\"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is too large (4.2 Gigabytes compressed), we load it into Spark. However, it is inefficient to process gzip-compressed CSV files directly with Spark due to their non-splittable nature, and using an unziped CSV file is not always splittable. As shown in the code below, the CSV file took over 7 minutes to run, which is a considerable time.\n",
    "\n",
    "```spark.read.csv``` evaluates lazily, which means that the read only occurs when an action is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 15:48:21 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "[Stage 1:======================================================>(260 + 4) / 264]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ROW_ID: integer (nullable = true)\n",
      " |-- SUBJECT_ID: integer (nullable = true)\n",
      " |-- HADM_ID: integer (nullable = true)\n",
      " |-- ICUSTAY_ID: integer (nullable = true)\n",
      " |-- ITEMID: integer (nullable = true)\n",
      " |-- CHARTTIME: timestamp (nullable = true)\n",
      " |-- STORETIME: timestamp (nullable = true)\n",
      " |-- CGID: integer (nullable = true)\n",
      " |-- VALUE: string (nullable = true)\n",
      " |-- VALUENUM: double (nullable = true)\n",
      " |-- VALUEUOM: string (nullable = true)\n",
      " |-- WARNING: integer (nullable = true)\n",
      " |-- ERROR: integer (nullable = true)\n",
      " |-- RESULTSTATUS: string (nullable = true)\n",
      " |-- STOPPED: string (nullable = true)\n",
      "\n",
      "+------+----------+-------+----------+------+-------------------+-------------------+-----+-----+--------+--------+-------+-----+------------+-------+\n",
      "|ROW_ID|SUBJECT_ID|HADM_ID|ICUSTAY_ID|ITEMID|          CHARTTIME|          STORETIME| CGID|VALUE|VALUENUM|VALUEUOM|WARNING|ERROR|RESULTSTATUS|STOPPED|\n",
      "+------+----------+-------+----------+------+-------------------+-------------------+-----+-----+--------+--------+-------+-----+------------+-------+\n",
      "|   788|        36| 165660|    241249|223834|2134-05-12 12:00:00|2134-05-12 13:56:00|17525|   15|    15.0|   L/min|      0|    0|        NULL|   NULL|\n",
      "|   789|        36| 165660|    241249|223835|2134-05-12 12:00:00|2134-05-12 13:56:00|17525|  100|   100.0|    NULL|      0|    0|        NULL|   NULL|\n",
      "|   790|        36| 165660|    241249|224328|2134-05-12 12:00:00|2134-05-12 12:18:00|20823|  .37|    0.37|    NULL|      0|    0|        NULL|   NULL|\n",
      "|   791|        36| 165660|    241249|224329|2134-05-12 12:00:00|2134-05-12 12:19:00|20823|    6|     6.0|     min|      0|    0|        NULL|   NULL|\n",
      "|   792|        36| 165660|    241249|224330|2134-05-12 12:00:00|2134-05-12 12:19:00|20823|  2.5|     2.5|    NULL|      0|    0|        NULL|   NULL|\n",
      "+------+----------+-------+----------+------+-------------------+-------------------+-----+-----+--------+--------+-------+-----+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file_path = \"dataset/CHARTEVENTS.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve performance, we load the dataset into Spark using the Parquet file format with Snappy compression, ensuring splittable and efficient parallel processing across multiple nodes in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file_path = \"dataset/CHARTEVENTS.parquet\"\n",
    "\n",
    "df.write.format(\"parquet\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .save(parquet_file_path)\n",
    "\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's instant!?  \n",
    "Actually for this format, the data is read lazily and so is only read when required.  \n",
    "Parquet is columnar storage, which means it organizes data by columns rather than by rows, and this difference affects how data is displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to repeat the process for \"ICUSTAYS\" and \"D_ICD_DIAGNOSES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ROW_ID: integer (nullable = true)\n",
      " |-- SUBJECT_ID: integer (nullable = true)\n",
      " |-- HADM_ID: integer (nullable = true)\n",
      " |-- ICUSTAY_ID: integer (nullable = true)\n",
      " |-- DBSOURCE: string (nullable = true)\n",
      " |-- FIRST_CAREUNIT: string (nullable = true)\n",
      " |-- LAST_CAREUNIT: string (nullable = true)\n",
      " |-- FIRST_WARDID: integer (nullable = true)\n",
      " |-- LAST_WARDID: integer (nullable = true)\n",
      " |-- INTIME: timestamp (nullable = true)\n",
      " |-- OUTTIME: timestamp (nullable = true)\n",
      " |-- LOS: double (nullable = true)\n",
      "\n",
      "+------+----------+-------+----------+--------+--------------+-------------+------------+-----------+-------------------+-------------------+------+\n",
      "|ROW_ID|SUBJECT_ID|HADM_ID|ICUSTAY_ID|DBSOURCE|FIRST_CAREUNIT|LAST_CAREUNIT|FIRST_WARDID|LAST_WARDID|             INTIME|            OUTTIME|   LOS|\n",
      "+------+----------+-------+----------+--------+--------------+-------------+------------+-----------+-------------------+-------------------+------+\n",
      "|   365|       268| 110404|    280836| carevue|          MICU|         MICU|          52|         52|2198-02-14 23:27:38|2198-02-18 05:26:11| 3.249|\n",
      "|   366|       269| 106296|    206613| carevue|          MICU|         MICU|          52|         52|2170-11-05 11:05:29|2170-11-08 17:46:57|3.2788|\n",
      "|   367|       270| 188028|    220345| carevue|           CCU|          CCU|          57|         57|2128-06-24 15:05:20|2128-06-27 12:32:29|2.8939|\n",
      "|   368|       271| 173727|    249196| carevue|          MICU|         SICU|          52|         23|2120-08-07 23:12:42|2120-08-10 00:39:04|  2.06|\n",
      "|   369|       272| 164716|    210407| carevue|           CCU|          CCU|          57|         57|2186-12-25 21:08:04|2186-12-27 12:01:13|1.6202|\n",
      "+------+----------+-------+----------+--------+--------------+-------------+------------+-----------+-------------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "icu_file_path = \"dataset/ICUSTAYS.csv\"\n",
    "\n",
    "df_icustays = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(icu_file_path)\n",
    "\n",
    "df_icustays.printSchema()\n",
    "df_icustays.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_parquet_file_path = \"dataset/ICUSTAYS.parquet\"\n",
    "\n",
    "df_icustays.write.format(\"parquet\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .save(icu_parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ROW_ID: integer (nullable = true)\n",
      " |-- ICD9_CODE: string (nullable = true)\n",
      " |-- SHORT_TITLE: string (nullable = true)\n",
      " |-- LONG_TITLE: string (nullable = true)\n",
      "\n",
      "+------+---------+--------------------+--------------------+\n",
      "|ROW_ID|ICD9_CODE|         SHORT_TITLE|          LONG_TITLE|\n",
      "+------+---------+--------------------+--------------------+\n",
      "|   174|    01166|TB pneumonia-oth ...|Tuberculous pneum...|\n",
      "|   175|    01170|TB pneumothorax-u...|Tuberculous pneum...|\n",
      "|   176|    01171|TB pneumothorax-n...|Tuberculous pneum...|\n",
      "|   177|    01172|TB pneumothorx-ex...|Tuberculous pneum...|\n",
      "|   178|    01173|TB pneumothorax-m...|Tuberculous pneum...|\n",
      "+------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_icd_diagnoses_file_path = \"dataset/D_ICD_DIAGNOSES.csv\"\n",
    "\n",
    "df_d_icd_diagnoses = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(d_icd_diagnoses_file_path)\n",
    "\n",
    "df_d_icd_diagnoses.printSchema()\n",
    "df_d_icd_diagnoses.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_icd_diagnoses_parquet_file_path = \"dataset/D_ICD_DIAGNOSES.parquet\"\n",
    "\n",
    "df_d_icd_diagnoses.write.format(\"parquet\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .save(d_icd_diagnoses_parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ROW_ID: integer (nullable = true)\n",
      " |-- SUBJECT_ID: integer (nullable = true)\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- DOB: timestamp (nullable = true)\n",
      " |-- DOD: timestamp (nullable = true)\n",
      " |-- DOD_HOSP: timestamp (nullable = true)\n",
      " |-- DOD_SSN: timestamp (nullable = true)\n",
      " |-- EXPIRE_FLAG: integer (nullable = true)\n",
      "\n",
      "+------+----------+------+-------------------+-------------------+-------------------+-------+-----------+\n",
      "|ROW_ID|SUBJECT_ID|GENDER|                DOB|                DOD|           DOD_HOSP|DOD_SSN|EXPIRE_FLAG|\n",
      "+------+----------+------+-------------------+-------------------+-------------------+-------+-----------+\n",
      "|   234|       249|     F|2075-03-13 00:00:00|               NULL|               NULL|   NULL|          0|\n",
      "|   235|       250|     F|2164-12-27 00:00:00|2188-11-22 00:00:00|2188-11-22 00:00:00|   NULL|          1|\n",
      "|   236|       251|     M|2090-03-15 00:00:00|               NULL|               NULL|   NULL|          0|\n",
      "|   237|       252|     M|2078-03-06 00:00:00|               NULL|               NULL|   NULL|          0|\n",
      "|   238|       253|     F|2089-11-26 00:00:00|               NULL|               NULL|   NULL|          0|\n",
      "+------+----------+------+-------------------+-------------------+-------------------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"dataset/PATIENTS.csv\"\n",
    "\n",
    "df_patients = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "df_patients.printSchema()\n",
    "df_patients.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_file_path = \"dataset/PATIENTS.parquet\"\n",
    "\n",
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "\n",
    "df_patients.write.format(\"parquet\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .save(patients_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
